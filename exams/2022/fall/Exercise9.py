import numpy as np


# LDA function
def LDA(X, y):
    """
    Linear Discriminant Analysis.

    A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayesâ€™ rule.
    Assumes equal priors among classes

    Parameters
    ----------
    X : array-like of shape (n_samples, n_features)
        Training data
    y : array-like of shape (n_samples,)
        Target values.

    Returns
    -------
    W : array-like of shape (n_classes, n_features+1)
        Weights for making the projection. First column is the constants.

    Last modified: 11/11/22, mcbo@dtu.dk
    """

    # Determine size of input data
    n, m = X.shape
    # Discover and count unique class labels
    class_label = np.unique(y)
    k = len(class_label)

    # Initialize
    n_group = np.zeros((k, 1))  # Group counts
    group_mean = np.zeros((k, m))  # Group sample means
    pooled_cov = np.zeros((m, m))  # Pooled covariance
    W = np.zeros((k, m + 1))  # Model coefficients

    for i in range(k):
        # Establish location and size of each class
        group = np.squeeze(y == class_label[i])
        n_group[i] = np.sum(group.astype(np.double))

        # Calculate group mean vectors
        group_mean[i, :] = np.mean(X[group, :], axis=0)

        # Accumulate pooled covariance information
        pooled_cov = pooled_cov + ((n_group[i] - 1) / (n - k)) * np.cov(X[group, :], rowvar=False)

    # Assign prior probabilities
    prior_prob = n_group / n

    # Loop over classes to calculate linear discriminant coefficients
    for i in range(k):
        # Intermediate calculation for efficiency
        temp = group_mean[i, :][np.newaxis] @ np.linalg.inv(pooled_cov)

        # Constant
        W[i, 0] = -0.5 * temp @ group_mean[i, :].T + np.log(prior_prob[i])

        # Linear
        W[i, 1:] = temp

    return W


# Define class means and a dummy dataset for training the LDA
# means = np.array([[24, 3], [30, 7]])
# n_samples_per_class = 100
# X_dummy = np.vstack([np.random.randn(n_samples_per_class, 2) + mean for mean in means])
# y_dummy = np.hstack([np.ones(n_samples_per_class), np.zeros(n_samples_per_class)])

# Train LDA
# W = LDA(X_dummy, y_dummy)

# Sample point x
# x = np.array([23, 5])

# Calculate the discriminant value for the sample
# y_C2_x = x @ W[1, 1:] + W[1, 0]

# Determine class based on threshold
# class_assignment = "Class 2" if y_C2_x > 0 else "Class 1"

# print(W, y_C2_x, class_assignment)

#

x = np.array([[24, 3], [30, 7]])
y = np.array([[23, 5]])

r = LDA(x, y)
print(r)
